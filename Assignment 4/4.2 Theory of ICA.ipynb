{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2 Theory of ICA\n",
    "\n",
    "### 4.2.1 Difference between _uncorrelated_ and _statistical independence_\n",
    "\n",
    "The goal of this part is to understand the difference between _uncorrelated_ and _statistical independent_ data sets.\n",
    "\n",
    "In the previous part, you have seen that principal component analysis (PCA) finds a set of __orthogonal (uncorrelated)__  basis vectors pointing in the directions of maximal variance of the data set. Formally, two random variables X,Y are defined to be uncorrelated $:\\Longleftrightarrow$ their covariance $\\bf \\operatorname{cov}(X,Y) = E(XY) − E(X)E(Y)$ is zero. Now, remember the Pearson correlation coefficient:\n",
    "\n",
    "\\begin{equation} \\bf \\rho_{X,Y}= \\frac{\\operatorname{cov}(X,Y)}{\\sigma_X \\sigma_Y} \\end{equation}\n",
    "If you compare the definition of uncorrelated and the definition of the correlation coefficient, you can easily see that two random variables with non-zero variance are uncorrelated $\\Longleftrightarrow$ their correlation coefficient is zero. \n",
    "\n",
    "Independent component analysis (ICA) on the other hand is trying to find a set of basis vectors which are __statistically independent__. On an intuitive level, statistical independence between $\\mathbf X$ and $\\mathbf Y$ means that information about the outcome of $\\mathbf X$ does not change the probability of the outcome of the other variable $\\mathbf Y$. More rigorously, two random variables $\\mathbf X$ and $\\mathbf Y$ with cumulative distribution functions $F_X(x)$ and $F_Y(y)$, and probability density function $f_X(x)$ and $f_Y(y)$, are independent $\\Longleftrightarrow$ the combined random variable $(\\mathbf X, \\mathbf Y)$ has a joint cumulative distribution function $F_{X,Y}(x,y)$ satisfying:\n",
    "\n",
    "\\begin{equation} F_{X,Y}(x,y) = F_X(x) F_Y(y) \\end{equation}\n",
    "\n",
    "__Task:__ Review the concepts of the correlation coefficient and statistical independence and answer the following question:\n",
    "\n",
    "#### Q4.2.1 Which of the data sets in (x,y) are uncorrelated? Which are statistically independent? Which of the two properties is stronger?\n",
    "\n",
    "__Hint:__ The correlation coefficient is zero (and thus X, Y are uncorrelated) if the slope of a linear fit is zero.\n",
    "\n",
    "![title](data/Independence.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a correlated (Pearson=1)\n",
    "\n",
    "1b\n",
    "\n",
    "1c\n",
    "\n",
    "1d uncorrelated (Pearson = 0), statistically independent\n",
    "\n",
    "2x uncorrelated (Pearson = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Basics about ICA\n",
    "\n",
    "Next, we want to cover the basics of independent component analysis (ICA). ICA is a computational method for separating a multivariate signal (e.g. sounds) into (independent) additive subcomponents (e.g. speakers or singers). \n",
    "\n",
    "The standard model for ICA for the same number of sources and signals is:\n",
    "\n",
    "\\begin{equation} \n",
    "\\mathbf x = \\mathbf {A s}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\bf x=(x_i,...x_D)$ is a column vector with the observations (e.g. in time) $\\bf x_i \\in R^T$, $\\bf s=(s_1,...s_D)$ is the source vector containing the original components $\\bf s_i \\in R^T$ and $\\mathbf A \\in \\bf R^{DxD}$ is a linear transformation that defines the linear mixing of the constituent signals. Typically, $\\mathbf A$ and $\\mathbf s$ are unknown. \n",
    "\n",
    "__Task__: Review the lecture slides or other literature about ICA and answer the following questions:\n",
    "\n",
    "#### 4.2.2 What are the central assumptions for the ICA to work? \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Ambiguities in ICA\n",
    "\n",
    "In comparison to PCA, ICA has two major ambiguities:\n",
    "\n",
    "1. We cannot determine the variances (energies) of the independent components.\n",
    "2. We cannot determine the order of the independent components.\n",
    "\n",
    "#### Q4.2.3 Why do these ambiguities occur? __Hint__: Answer this questions by looking at the standard model $\\mathbf x = \\mathbf {As}$.\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4  FastICA and negentropy\n",
    "\n",
    "In exercise 4b.2, you will use an implementation of ICA which is called FastICA. It is an iterative fixed-point scheme which is minimizing the entropy (or equivalently maximizing the negative entropy (negentropy)) in order to find the independent components. \n",
    "\n",
    "The entropy of a random variable can be interpreted as the degree of information that the observation of the variable gives. The more “random”, i.e.  unpredictable and unstructured the variable is, the larger its entropy. \n",
    "\n",
    "#### Q4.2.4 Find out which special property the normal distribution has with respect to entropy for a fixed variance. How can this property be used for finding the independent components?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5 Preprocessing\n",
    "\n",
    "Before applying ICA, most algorithms expect you to preprocess your data. Commonly, two preprocessing steps are applied before performing an ICA: __Centering__ and __Whitening__.\n",
    "\n",
    "__Centering__ is done by simply subtracting the sample mean, i.e. $\\bf x_c = x - \\bar x$. This is equivalent to a translation of the data such that they are positioned around the origin.\n",
    "\n",
    "__Whitening__ is a transformation that converts the data such that it has an identity covariance matrix. Let $\\hat \\Sigma$ be the estimated covariance matrix. Then $\\bf x_w = \\hat \\Sigma^{-\\frac{1}{2}} x_c$ would be the new whitened vector with zero mean and identity covariance matrix. \n",
    "\n",
    "One popular method for whitening is to use the eigenvalue decomposition (EVD) of the covariance matrix: $\\bf \\hat \\Sigma = EDE^T$ where ${\\bf E}$ is the orthogonal matrix of eigenvectors of $\\hat \\Sigma$ and ${\\bf D}$ is the diagonal matrix of its eigenvalues, ${\\bf D}= \\mbox{diag}(d_1,...,d_D)$. Whitening can now be done by computing\n",
    "\n",
    "\\begin{equation} {\\bf x_w}={\\bf E}{\\bf D}^{-1/2}{\\bf E}^T {\\bf x_c} \\end{equation} \n",
    "\n",
    "where the matrix ${\\bf D}^{-1/2}$ is computed by a simple component-wise operation as ${\\bf D}^{-1/2}=\\mbox{diag}(d_1^{-1/2},...,d_D^{-1/2})$.\n",
    "\n",
    "Below you see an implementation and the effect of centering and whitening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "# Create normally distributed data-set\n",
    "N_samples = 200;\n",
    "X=np.random.multivariate_normal([1,-2],[[1,0.9],[0.9,1]],N_samples)\n",
    "\n",
    "# Centering\n",
    "X_meanfree = X - np.mean(X,axis=0) \n",
    "\n",
    "# Whitening\n",
    "X_cov = np.cov(X.T)\n",
    "[D,E] = np.linalg.eig(X_cov)\n",
    "D = np.diag(1/np.sqrt(D))\n",
    "X_white = np.dot(np.dot(np.dot(E,D),E.T),X_meanfree.T).T\n",
    "#print(np.cov(X_white.T)) #Should be identity\n",
    "\n",
    "#Draw the figure\n",
    "jtplot.style('onedork')\n",
    "f, ax = plt.subplots(1, 3,figsize=(20, 6))    \n",
    "for k in range(3):\n",
    "    if k==0:\n",
    "        X_temp = X\n",
    "        ax[k].set_title('Original Data Set',fontsize = 14)\n",
    "    elif k == 1:\n",
    "        X_temp = X_meanfree\n",
    "        ax[k].set_title('Mean Free Data Set',fontsize = 14)\n",
    "    elif k == 2:\n",
    "        X_temp = X_white \n",
    "        ax[k].set_title('Whitened Data Set',fontsize = 14)           \n",
    "    \n",
    "    #X_temp = np.array(X_temp)\n",
    "    ax[k].scatter(X_temp[:,0],X_temp[:,1])\n",
    "    ax[k].set_xlim([-5,5])\n",
    "    ax[k].set_ylim([-5,5])\n",
    "    ax[k].grid()\n",
    "    ax[k].set_xlabel('$X_1$',fontsize = 14)\n",
    "    ax[k].set_ylabel('$X_2$',fontsize = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4.2.5 Why is it useful to whiten the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
